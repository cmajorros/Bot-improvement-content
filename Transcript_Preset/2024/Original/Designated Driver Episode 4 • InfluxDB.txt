 Music Grab a beer, take a seat Time to tune and feel the beat Data faces an open source Designated drivers, days on course Designated driver, here we go While we're drinking, start the show Data faces, we'll explore Open source in so much more Music Hello everybody and welcome to Designated driver. This is our What fourth episode and today we have a special guest on I East from inflexdb and Let's start off with doing a little bit of Introductions. I'm Evan Rosakis, I am a National beer judge and a Data viz nerd and work for Preset, which is the hosted Commercial version of a Pepsi Superset open source BI tool. It's awesome. You should try it. Beto, take it away. Thanks Evan. I'm Beto Jaiomeda. I work at Preset also, a software Engineer. I work all over The stack with Superset But I do a lot of work Connecting Superset to Different databases. That's Kind of right. Enjoy being in The stack. Yeah, I'm not a Beer judge, but I do like beer Also. And I used to do that. Yeah, I'm on a east. I'm a Developer advocate in Flex data. Been there for about Six years. I'm not a beer Judge. I don't usually know What beer I'm even drinking, but I know if I like it. So I guess I'm my own beer judge in that Way. So Right on. Yeah, they say the best Beer is the one in front of Yeah. Yeah. There we go. Well, thanks for joining us today For our little happy half hour. We like to keep this casual and Just kind of meet some people From different database communities And you know, kind of hear what Your product and what you are all About. And I guess for those Who don't know Apache Superset Just to start that off. We're an Open source business intelligence Tool and we connect to a whole A bunch of different databases, Quarrians, data sources of all Sorts and you know, we just Like to bring people on and hear About their use cases. So you Can give it a try and you know, Learn something new we hope. So I guess on a to start off Like can you give us a little bit Of your kind of background with the Project and how you got involved here? Yeah, sure. So yeah, like I Mentioned developer advocate for Influx data. So for those of you Who aren't familiar with what that Means that's just basically someone Who tries to represent the company To the community in the community to The company. And I do that Through creating POCs or demos With different technology stacks and Answering questions online, writing Technical blogs going on podcast Like this giving webinars and talks Just to help educate developers About the different options that are Available and how you could build a Specific architecture to solve Whatever problem you might have. So I'm here today because Influx DB3 which is our latest Version of our database is all Written on the Apache ecosystem. We use Apache parquet. Data fusion, flight, arrow, And you can connect to Apache Superset with the flight SQL protocol. So yeah, it's a Great way to basically visualize All of your data and create all the Dashboards that you need. And yeah, I think that's pretty Much time here today. Fantastic and for those who want to try This out by the way there is Documentation available on the Influx website so you can see how To connect to Superset with the Endips easy peasy. But we forgot to do one little bit Of round of the introductions which Is the beer we're supposed to be Having a beer while we do this. So and we try to make it thematic. So if you guys want to Optionally introduce your beer You're more than welcome to. Since today we're doing. You know, an episode about a time Series database about what's effectively A time series beer, which is a Belgian News, which is actually they brew With the same beer three years in a row And then blend it all together. So in a way, it's kind of a time Series. But anyway, maybe that's a Bit of a stretch, but. I like it. I like it. We have people in the community who Monitor their brewing at home Within flux. That's a set up controllers and stuff With that. So I've done projects Around that as well. So. Oh, that's awesome. Definitely in theme. Yeah. I got it. Cherry beer. Cherry. Shoe. Little known fact knowns they actually Love time series. So that's the relationship. I have feeling that cherry did. My beer is a hippie haze. I don't think that hippies pay a lot Attention to time. So we can use a time series database. I don't know. Fantastic. Hopefully as. As we do more and more episodes of this. If any of you beer companies are listening, You know, we're open to sponsorships. Just. There we go. The true agenda unfold. Cool. So. Yeah, I can you tell us a little bit about, You know, kind of. Time series database is in general and flux D. B. And how it fits into that space and kind of what are the The main advantages of your platform. Sure. So a time series database is going to be a database That prioritizes time series data and time series data is any Data that has a time stamp associated with it. So really when we think about time series databases, They need to be able to accommodate time series data. They also need to be a comedy. Really high throughput. And that's because. You're typically looking at monitoring something and that can be Either your virtual world or. The physical world, you know, Whether or not you're performing something like dev ops monitoring or Application monitoring. You're getting a bunch of time series data from, you know, CICD from endpoints from a user activity logs traces, etc. And then in the physical world, we're thinking about things like IoT. So anytime that we have a sensor collecting any data about our physical environment, Whether that's pressure, temperature, concentration, flow rate, you name it. That's all time series data and. Specifically, if we look at, for example, Like a vibration sensor that can produce like 10,000 kilohertz of data per second. Yes, which is around 10,000 points per second of data. And if you have multiple sensors like that on a factory floor, Then you quickly are, you know, consuming hundreds of thousands of points per second. So you want something that can, can effectively accommodate that throughput. And then the other thing that worry also consider is that you have to have good query performance, right? Because if you can't query on that data, what's the point of even being able to have access to that high throughput. And then last but not least, and this isn't really unique to time series per say, But you know, you want it to be scalable and distributed so that you can also reach that performance level and that the throughput requirements. And influx DB specifically, it's like I said, it's written on the Apache ecosystem. It's all changed to call them are format as well. Call them are format is really well suited to time series. Because it, you know, it offers compression advantages and dictionary, which is really useful. And then a big reason why we completely rewrote the storage engine in three. And then we went to not only accommodate higher performance in terms of writing and dimensionality, but also interoperability of other tools like superset. In previous versions of influx, we had everything kind of out of the box, which is great. It was a one stop shop for all your needs. But as you might imagine, we can't cover every single use case, just one company trying to build data engine or data processing engines. Visualization tooling, et cetera. So we made this move to being based on the Apache ecosystems that we can offer in top ability with other tools that are also leveraging arrow also leveraging flight. And being able to contribute to those projects upstream so that we can be part of that open data architecture. That's awesome. The part of the kind of the entire Apache stack, if you will. Like hearing about that sort of integration. I'm curious. You know, you mentioned some pretty different use cases. There's, you know, the factory floor with all these kind of high speed sources of data flowing through and then, you know, then the humble home brewer with probably some little, you know, monitor sending temperature and, you know, should or specific gravity or whatever. How often are people dealing with that kind of super high throughput stuff that leads to sort of scaling issues or do people tend to have kind of these, I guess you would say easier to deal with use cases of, you know, time series data. Definitely our customers deal with the high throughput challenges, luckily, because we have an open source version, not a V3 yet that's coming later this summer. I don't see as many of those and I do work mostly with people doing cool projects at home, cool IoT projects, maybe some home assistant projects. I once met someone who was using influx DB to track endangered Falcons, which was a, yeah, like another really cool project. So yeah, I don't encounter I personally don't encounter that much, but if you were to talk to our sales engineers. Oh, yeah, they definitely do. Here's about like data ingestion, like if I build the system, I used to grow. So let's say I have some sense, some sensors I want to send that to an influx DB database. Is there like a recipe I can send the data. Like, how does the ingestion work? I assume like it must be robust, right? So you're not losing data if you're using it for IoT and you have high frequency data. Yeah, for sure. So we definitely have a right API and that's been the same for all three versions. So that's one benefit if you're migrating from whatever version. It's the exact same. The query API is now just leveraging flight SQL under the hood. So there's that. But then we also have telegraph and telegraph is our collection agent for metric and events. It's plug and driven. It's also open source. And it supports a lot of different IoT protocols, OPC UA, MQTT, MOBAs, SNMP. I think I listed all the ones I can think of right now. So yeah, you can use telegraph easily configure that with a variety of those input plugins for your IoT data and then ship it to an influx DB. And telegraph is also cool because you can build the binary with just the plugins you want so that if you want a side card, side card to it can be much more efficient and smaller. Yeah, it's also really easy to use and it has you know the buffing buffering and caching capabilities. So if you know something at some endpoints unavailable, you can, you know, be feel safe. But also, you know, we also try and provide integration with other data engineering and data streaming tools data pipining tools like quicks, for example, we're a recent partner with them. And they basically provide an interface and Python abstraction for working with Kafka. So you could also leverage something like that to also get some of your IoT data and send that with an MQTT client, for example, through them. And then do any sort of transformations that you might want to do. So yeah. See, yeah, I'm a big fan of just monitoring collecting data. I have my own MQTT service at home that I used to automate everything. So good. Yeah, we both do a lot of IoT tinkering. So I'm already thinking a lot of these cases right now. Yeah, I'm like, hmm. Need to add more to my infrastructure at home. Yeah, I also had a friend who worked on a solution to automatically add more chlorine to his pool with in blocks. I thought that was like a really cool practical use case to. Interesting. Is there some way of setting like alerts? So you have recurring quick queries. And if you hit the trash or there's something you get notified or that's, that's delegated to the rest of this, the Apache stack. That's delegated to you to any other bi tool that you want to use for alerting most commonly users are typically gravitate towards graphana and they leverage graphana's alerting capabilities, but yeah, you could use whatever you wanted. You super set. We talked a little bit about the use cases of, you know, how to use it, but I'm curious about the community side of things and those who are contributing. Are there any kind of particular orgs or teams or individuals that are back in the open source side of the project. So we used to have an influx ACES program, which we basically highlighted some community members who were extra active, either in. Just the community forums and Slack and participating there or, you know, maybe contributed technical blog posts on the projects that they have worked on themselves. That has been on pause for now because we want to wait for the V3 open source before rebooting that. But we definitely have a lot of contributors on the telegraph side for sure. I think, like 90% of the plugins have been contributed to by the community and we have really clear contribution guides on that. So that's been really cool to see. It's also just a great place if you're looking kind of like for a new developer as a first kind of project to contribute to. There's also these plugins that are called the exact plugins and they make telegraph extensible in the language of your choice. So we also have these what we call external plugins where you can just leverage as exact plugins, write a script in whatever language you want and collect or process for right data to whatever source you want and contribute those plugins as well. And that that's also extremely easy to do. So we see most, yeah, most of the contribution on that side, I'd say. Because inflex DB is in rest, right? Yes. It's within rest. Yeah, the original name for V3 was Iox because it's the chemical symbol for us. Yeah. Nice. That's cool. I feel like plugins are a really good way of getting contributors, you know, because the scope is this usually contain is like, okay, I don't need to understand the whole project. I just want to look again. It's a really great way to get more developers to get people started contributing. But like for something like super set, you have to use Python. If you want to do a database plugin. I think not not forcing people to use a specific programming language is a big plus. Yeah, for sure. Yeah, we're trying to get our plugin game together on the front end to, and I know this is not a front end conversation per se, but, yeah, there's different places for plugins all throughout super set and we're trying to make that more approachable so that more people can contribute these little add-ons. But I'm curious with all these people contributing what's your kind of role in that in terms of a developer advocate or how do you see developer advocacy in your community? So with regards to contributing specifically, I think that starts with just answering people's questions because usually before they even want to contribute, they're just trying to solve a problem for themselves. And so taking an active interest on the forms and slack about not just answering their question, but also probing a little bit deeper and being like, hey, what are you doing with influx? Like what's the bigger problem that you're trying to solve? And then getting excited about their project and kind of encouraging them to share their work because it'll be a surprise how many times you were just doing something for themselves and they just kind of assume that, oh, this is just a me thing, but really it'd be really useful for other people to have exposure to and to share. So just, yeah, just, and it makes my job so much more interesting too when I'm not just answering questions and resolving errors, but understanding like the full context and what someone's trying to do. And that's when you find little gems about, you know, someone monitoring their beer or monitoring endangered falcons or helping their pool or whatever might be monitoring their barbecue as well as that's another one that's kind of common. And then it just reminds you that like when you're answering all these questions on forums that there's, you know, bigger picture and people are doing fun creative things with it so. Well, speaking of monitoring beer, how is yours? It's good. It's a little very happy, but I think that's to be expected. Yeah. I'm a secret too sweet for my taste. I knew it was going to be sweet. I only have myself to blame. I'm sad that hours are disappearing because I really love hours and I feel like there's a craze for a while now they're just kind of going away. Well, yeah, they're still out there. I mean, I guess I'm having one now, which is unusual for me, I admit, but today is the day, but they're also a little spendy. Like there's not a whole lot of, you know, very affordable hours. The good ones get kind of fancy. I have a little bit also right. That's true. Nobody wants to bring the bits 750 bottle of sour to themselves. Yeah, I really like sour. Cool. And then actually while we're off the topic of databases, I've got to ask the million dollar question here. Do you have anything fun on your desk that you could show us? I do. I have this little stress toy cat thing that you can squeeze the heck out of it. And it's a cat. It's a cat. Yeah, that's it. Yeah. And then I have sort of like pop thing. Oh, yeah. Oh, yeah. I like those. We got a couple of those around the house. What about y'all? Oh, good question. I, right now, let's see, I've been making a lot of things out of sculpy, which is like this polymer clay you can bake in the oven. So I've been starting to make a little Pokemon. But there's most of them are out in the other room. That's just the one I happen to have on my desk. Yeah, I have this little guy like advancing. I got it in my stocking last. That matter. You like it? I love how some of these trinkets like some of them just come and go from your life, but others just stay like randomly like friendships do like it's just you never know which one's just going to forever. It's true. Some of them just keep turning up. Well, let's see. So you've got the new version. What else is kind of on your roadmap at the moment? What are you looking forward to being either built or shipped? Yeah, I'm definitely really looking forward for the open source version to be available so that then we can finally start migrating some of the open source V2 users to it and see how they like it and what how they feel about. Some of the performance aspects because another big headache in previous versions was worrying about cardinality and your dimensionality exploding because of the way that. Tags and fields were indexed and so now you don't have to worry about that you don't have to spend effort creating tasks to, you know, down sample your data or reduce the cardinality in some shape reform. So I'm excited to see people feel the relief from some of those headaches and I think also similarly there's work being done with iceberg and to help create custom partitioning of the parquet files. And so I'm excited to see what sort of performance that offers and there's also loose plans, very loose plans around how and in what versions accessing the parquet files directly will. Be available and that's really exciting from a lack of like vendor lock in perspective and just being able to take those parquet files and use them directly with other machine learning tools machine learning libraries, etc. So that to me is really exciting. Yeah, and just like all the doors at that opens. Yeah, it's interesting you mentioned cardinality can superset started as a Y for druid the database and and a lot of the semantically in superset was made to prevent people from doing, you know, group eyes and columns that have the high cardinality. So that's all the semantic bear started you basically would annotate like this column has the cardinality of this column has high cardinality and then it would prevent you from grouping by the column with the high cardinality because it would yeah it would just explode also. Are there any features that you guys are excited about on superset. Yeah, I am I finally did two features that I've been wanting to do for a couple years now. One of them is all integration. So for databases like BigQuery, Google sheets is popular database also handle it's not technically a database, but like the query snowflake Databricks, Treno. You can you can do like authenticate with using all of. And the other feature is support for catalogs. So traditionally superset you connect to the database, you have to specify your catalog and then you can browse schemas but can't browse catalogs and we just merge the it was. So you can make a series of like five or six big PRs refactoring everything so that now you when you connect there's there's still a default catalog when you can switch catalogs. To crack queries great data sets, visualizations. And I joke because it was such a huge refactoring I think it took me two months and like is it six PRs each one touching like 60 files. And the end result is that sometimes you get an extra drop down into UI that's that's what I have to show is like, oh, you might have a drop down here where you can choose your catalog and that was it. It looks so simple. There's some very small stuff like relevant to what you were just saying we just improved our support for uploading part files. So you know that might come in handy for yeah, folks, my personal one was yesterday. I finally after much ado merged a PR that added about a hundred country maps to our country map plugin. So we're everywhere now. Did you see that somebody was asking for the map of Paris. Oh, yeah, I did see that. So now you need to do every city or every capital. Yeah, I don't even know how to go back. You should just give it to Max. He lived in Paris was going to be excited. Well, cool. Are there any shout outs or anything you wanted to make to your community members or anything while we're at it? I guess I just want to also, I guess just plug in Flexib University, which is a place where you can get free and live trained. Free live and online training for all things in Flexib and we're currently adding more and more in Flexib B3 courses. So yeah, if you want to use that as a as another resource for learning about all things in Flex, please do. And then also the In Flex community. One word repo or organization on GitHub has so many examples of how to use In Flexib with a variety of different tools, including Super Set. So if you want any, you know, demos that you can leverage yourself there to get a feel for how to play with different different stacks. That's a great resource. Okay, cool. And as mentioned, there are documentation links out there that I'll put in the relevant places for this. And so people should try it all out, of course. And then you've got a slack community. We've got a slack community. So everybody gets slacken. And yeah, we'll talk online some more. Sounds good. Thank you. Well, thank you for being a part of this. We really appreciate you and everything you do and talk to you soon. Yeah, likewise. Thank you.