 Let me share my screen and tell you about what we're going to get into today. So this meetup is all about how we as superset users can use GPT and related products and technologies and making our daily lives a little better or easier. And kind of where this is all leading us, not just, you know, people of the world playing with a shiny new toy, but what can we do in our actual work with it? So I'll go ahead and introduce our panelists today very briefly. I'm Evan Rusakis. I do community work with the superset community doing dev rel dev advocacy, et cetera. We've got Diego Pucci, who is doing a lot of engineering work at preset as well and has some fun stuff. He's going to show you later. And we also have Maxime as well, our CEO and of course the original creator of superset. So thanks for joining today and each of us have a little segment we're going to walk you through. And before we then we get started to, like how many times a day do you two like use, you know, prompt or start a new session with chat GPT I interact with chat GPT every day. It's been like maybe like three or four sessions a day, like 12 to 20 prompts or so, like everything, everything that I do, I try to see whether it can help me in some intricate ways. I think that might be on the extreme of usage, you know, but Evan, yeah, I don't know the audience to like just like getting a sense of like how I'll often it's like I'm sure some people just play with it a little bit and kind of left it on the side. Like you're going to play with the toy and then, you know, put it back on the shelf, but you're still get a sense for how much people are using this stuff every day. Personally, it's probably just a few times a day on average, but at the last couple of weeks, it's through the roof. Prepare for this session, I guess. We'll see why. And actually, so what I've been working on that I wanted to cover today is some GPT based life hacks for superset users, some things that do and don't work. And then Diego is going to show us an amazing demo of some stuff that's coming down the pipeline for preset users. And Max is going to kind of give us some additional thoughts on other applications and some future use cases for this technology. So I wanted to invite you all to kind of join the discussion as much as possible. There's a few options to do that. Number one, you've got the Q&A tool on your Zoom. Hopefully it's enabled if not say something on the chat and we'll figure that out. But yeah, if you want to have some questions answered as we go through this by all means speak up, don't ask questions in the chat because they just drift by in the stream and we won't be able to kind of circle back to those very easily. If you have questions you want to go deeper on or want to discuss asynchronously, like they're not important right now, then please join us on Slack. There's a link for that and I've created a channel for this specific meetup. So here is the link to that as well as a link to a GitHub discussion. So we've got a lot of ideas we're going to be talking about today and you might have some of your own that are worthy of further documentation experimentation productization. So let us know what you think is going to be an important use case and we'll, you can vote on these things. All right. So just publish the blog and I want you guys to check that out. It's a fairly lengthy blog, pardon me. I was going to try to copy paste that link but that doesn't work. So yeah, check it out. It's a short URL. But this goes through a lot of what we're going to cover today in much greater depth. So with the examples you can copy paste and things like that. And what I did here is go through a whole bunch of use cases that I gathered via discussions with peers of all sorts and wanted to try out the basic tools that people have in their hands. We're using GPT 3.5, GPT 4 and Google's Bard just to kind of balance out the set list a little bit. And so I'm not using all the fancy IDE tools. I'm not using some of the alternative more indie GPT models. And I am using some of the web access features that you get as a beta user of GPT 4. But I didn't go crazy with plugins. I didn't use Bing because it forces you to use Edge, which I'm not going to do. And yeah, we basically just want to see what you can do casually with the UI. I just got access to the web plugin. I think that was something last week and it doesn't work. So at least for me, and I've seen the reports too on Twitter and anecdotal on Reddit Twitter that you look at this thing go and it adds a little widget that says, you know, you're querying the web, clicking on the site, loading a web page. But then I get a lot of failure messages. I think it's like stuff doesn't fit in the context, the prompt window or something like that. But I get just a lot of error messages when trying to use that. Yeah, I'm finding that with a lot of the solutions. And it kind of handles that in weird ways we might cover. But so I'm going to breeze through a lot of the experiments that I've tried. And I encourage you to try them as well. But I very subjectively rated each of them on what I consider a hope scale. Like is this something that's just a pure fail that's wasting my time or is it something that's really going to change my life? And I'm going to start doing this every day. They're usually somewhere in the middle. But hopefully that scale can help. So I had to start with the big one, which is using natural language to build SQL queries. And I was very pleasantly surprised to find out that it works really well for this. If you give GPT, particularly GPT4, some sample data, some context around your table name, what kind of database you're using, things like that. It actually is pretty easy to prompt it to use, in our case, Postgres and come up with a very specific query like you see here and get some reasonable results back that actually looked correct. So in this example, I gave it a specific question and it came back explaining the CTEs. It was using to generate and all of that. And honestly, this is pretty amazing. If you don't want to spend your whole day writing SQL, it can work. And we'll talk more about that in the future. Then converting from one is equal dialect to another. I was hoping this would be a magic bullet where if you had a query in big query, you would be able to just say, hey, convert this to Postgres and it would use the right kind of quotations and the right kind of time functions and things like that. And it does a pretty great job of explaining what those things are that you ought to do. But at the end of the day, it never really yielded working results. So it would miss a lot of aggregations and calculated columns that I wanted from my original query and it would just kind of give me some nonsense that usually didn't run. If it did run, you would get wildly different results from what you were seeking. So however, it does give you the tips like here are the time functions you should use and date functions and things like that. So it is useful to kind of give you the the dance steps you need. So that's interesting. So that's that's translating from a dialect to another, right? So I have a SQL query, it's a Postgres query converted to something else. I know it's good at if you start from natural language, just say, can you generate this query in Postgres dialect or in another dialect? It might be good from natural language to any dialect, but from dialect translating to a dialect, maybe it doesn't do as well. Yeah, that's exactly the surprise. I thought that would be a slam dunk for it, but not so much the case. But it does point in the right direction. And then yeah, you trying to do auto correction. So in this case, we know that everybody is typing queries, you screw something up, it's usually a comma or something very trivial. And you get this error and you need to troubleshoot it. So what if we could use GPT to kind of auto correct bad syntax? Turns out it doesn't actually work that great. It does tell you, again, it tells you about what you ought to fix, but it does a bad job of actually giving you a running query back. It tends to over correct and cut chunks out of your query and do weird stuff like that, trying to optimize it and thus breaking it has been my experience. But it does again point you in the right direction with its text. Here's some methodology on this one. Did you pass the error message along with the query or just, here's a query, there's something around what it go, guess what it is. Yeah, that's actually the next one. So if I should have put them in that order, now that you mentioned it. Or oh, sorry, I did. But yeah, this was basically doing exactly that. You run a query, you get an error back and can you use GPT to replace stack overflow by pasting that error message into it and saying, here's my query, here's the error I got, fix it for me. It fails 100% of the time. It always says, okay, I fixed it, job done. And then you run it and it just to get you new error. So you can paste that error and you can iterate on this all day long. And it will say, I fixed it, I fixed it, I fixed it. And it never actually fixes it. I got one query to actually run after trying this approach and it gave just egregiously wrong results, wrong columns and everything. So, and that was using GPT 3.5. So I think it just had as kind of a successful hallucination. Then just to try it, I took some queries that I knew were kind of slow, kind of long, kind of complex and tried to have it optimize them. I didn't have much hope for this to be honest, but at the end of the day, it just failed drastically. It took out a lot of the logic, not just speeding it up, but removing logic from a query is not a great idea. But it does give you some tips about kind of refactoring CTEs, optimizing your wear clauses, things like that. So you get some reasonable pointers. That's about the best of it. I want to figure you get indexing type strategy because one way to optimize query is to create the right bitmap, binary tree indexes are partitioning the schemes and stuff like that. Then you need to have a corpus of query that are representative of the actual workload. So there you have to feed it probably more information. Yeah, probably. And it's prone to end up, but it might be able to help you design your covering indexes or that's interesting. It did allude to, hey, check what you're indexing. That's going to help a lot, which is somewhat helpful. But there's more to it than that. But another use case, which is the inverse of that natural language to SQL one that I thought would be interesting is taking a complex query and having explain exactly what it's doing. And you can see the long screenshots of some of these explanations on the blog post. But at the end of the day, I keep saying at the end of the day, but Bard actually did a fantastic job of this. And I was pleasantly surprised by that that not only did it explain what the query is doing very mechanically like GPT does, but this one actually gave me the context. Like this gave me the exact use case of why I wrote this query. And I thought that was intriguing. So it kind of guessed my workflow very accurately. Then linting actually had tip to max for mentioning this. He was talking about how, you know, in some queries, you might have things like unnecessary quotes or you might have a bunch of line breaks you do or don't want in your SQL. And if you give it very pointed instructions of, hey, leave the query as is except make this tweak to it. It can do a lot of that automation for you. So you don't have to sit there hand editing all this stuff and potentially breaking things. But you need to check its work like I've been alluding to. So this is where, you know, if you have some kind of means to get a get diff on what it did to your query, that's going to probably save you a lot of pain. And some comments on that the stuff you might want to do is like capitalize or decapalize the reserve words you might want to use like two in dense instead of four in dense. I did things around like please don't use commentable expressions. Those are the width clauses at the top of like, hey, I prefer the style of like in line sub query is supposed to be in direction of a CTE or people might like it the other way. So it can do that. And if you think of a set of linting rules that are really clear like, hey, I want inline comments or I want to find description what the query does, you might be able to industrialize that and say like, I'm going to have a prompt of a certain kind. That's like that says what my nomenclature or what my rules are and then go through say a DBT repo and airflow repo and say like, you file by file kind of lint my stuff and it might do a pretty darn good job at some of that. And I totally on the Python size I used, I use GPT a lot as an assistant to write, you know, Python functions mostly for you. Tills, these more functional functions that are limited in scope and now are very targeted. And then it will be really good at, you know, applying linting rules, writing, adding type in to your Python code, adding docs, string, that kind of stuff. So it's a, it's a good set of use cases to work on, call it like code standards or code quality in general, this very well at that. Well, I'm glad you brought that up. We're going to get to that. So unfortunately, time to have got one section in the middle here. Going to is actually about documentation. Like any open source product, we don't have absolutely everything covered in documentation. That's very hard to maintain. So there are times you might want to contribute to the documentation or just kind of fill in the gaps for your own knowledge. So I just tried a few of the use cases we hear a lot about on the community Slack. One, which is like, what do all these are back things do? We have a very complex R back system. Not everybody knows what all the permissions are for or which ones they should use to do a certain thing. So I tried to use GPT for with web access, hoping that it would, you know, know all the latest bits of documentation and all of that. But it basically hallucinated a bunch of things. And, you know, among the small handful of truths were a bunch of non-truths. And that's when you're playing with something like this, it's not just wrong. It's dangerously wrong. Like if you could screw something up in your world of security by doing the wrong thing here. So don't. Then, feature flags. Similar thing. People always want to know what these do. And if you ask it to just kind of tell you what's going on with superset feature flags, what's new, what are some important ones that I should configure, then again, you're going to get a little bit of truth, a little bit of lies and some stuff that's in the middle that's kind of right. Everything sounds very reasonable and it's very confidently wrong, which is annoying. So I did want to give a plus one to Bard here, which is why it's not a two, but a three on the hope scale that Bard was able to actually tell me about a relatively recent flag generic chart axes and like what that flag is for and it explained very well about categorical axes and things like that. There's like a core limitation, you know, to these to these to specifically like chat GPT and the GPT models is that it's training with a corpus that was prepared and fixed as of I believe ball of 2021 to so if you ask it about superset flag, even if it learned about it and read the superset documentation, which it did, right? The rest API documentation that read and it has learned some things around that maybe it might hallucinate some things around that, but it's as of 2021. So a huge limitation. So now you're like, okay, I need to teach it or have it reread this stuff or provide context before I'm going to ask questions, but that context window is pretty limited. So I don't know anything since two time that happens since, you know, 2022 roughly. And in my context window, what you can tell me before asking me a question is about like 2000, 4000, 1000 tokens depending on which model you're using at a point in time. You know, is a pretty big limitation, even like prepping that context window of like I'm going to squeeze in what I think is the relevant part of the superset documentation before I ask you a question about it. Become becomes kind of kind of difficult and hard and the web plugin does a work super well. You know, so a lot of hits there on the negative front, barred seem to be much more up to date. Right. So if you ask it, like when's the last time you read the superset documentation, it's more recent. So Google is better at continuously training the same way they're probably good at continuously indexing the internet. Yeah. And I was, I was kind of hoping that in the case of GPT since it was trained a couple years ago that I would get a more truthful picture around that snapshot in time. So with some of these feature flags that it said existed, I had to kind of do a little fact checking and look at the get history and say like, okay, was this a feature flag at some point that just doesn't exist today? And no, they're made up. Was that three, five or four? Because like three, five is notorious for hallucinating that kind of stuff. But both both of them are hallucinated. Yeah, it's like one of them one is on the edge of their knowledge, you know, it just tends to listen to it, which is very dangerous. It looks as you said, it looks very reasonable. Like the feature flags that will make up our feature flags that could very well exist in superset. They seem credible. Yeah. I don't know. Yeah. To answer someone's question I saw on the chat, it was about building queries and explaining queries and how do we provide this schema. In the case of building a query, I was providing just a handful of rows of sample data that were exported as CSV. So you basically do a, you know, a select star and you can kind of copy paste a handful of rows into it. And that's enough to kind of seed it as far as explaining the query. I didn't give it any data. Just give it a big query and it kind of sussed it out. Yeah. It does not know, right? So that's another thing is like it doesn't know about non-public information. So you have to put that into the prompt or the windows. If you wanted to generate SQL and on a certain schema, you have to provide, you know, the create table statement or some form of documentation of what are the table names, what are the column names, what are the data types, though the data types are probably not as important. And what is maybe some column level statistics, things like either through sample data or histogram of the kind of value that you might find in these, uh, the tables. And that's when, when you provide all that context, that's when it does well at writing SQL. Yep. And one last thing from this section is about writing markdown. As a superset user, you might want to contribute to documentation. You might want to use one of the text widgets in your dashboard and to do so, you, you might have some text that you would like nicely formatted or have some little links inserted, things like that, turn this list of things into bullets, all that kind of TDM, it's actually pretty great at. Uh, so, you know, give that a whirl. The one thing shortcoming of this, though, is it doesn't yet accept rich text. That would be amazing if you could paste a block in and it's got bold words, italic words, links, things like that. And it just converted that, but it doesn't yet. If anybody hears about that being released, let me know. I'd be excited. Okay. I'll try to go fast because we're, I want to pass the mic, uh, but developer tooling, um, fixing flaky cypress tests. It's a big problem on superset that our CI has to get restarted all the time because a lot of our test are flaky. So it, it basically provided a lot of theories as to why these things are flaky. I tried to fix them, but at the end, the test failed so it didn't work. But it gives you some, some useful tips in terms of waiting for things and stuff like that. I opened a couple of PRs on this. If anybody wants to actually look closer at that code, same thing with speeding up tests. We have a couple of tests that take about a minute and a half to run, for example. Uh, and it gave feedback on parallelizing things, adding fixtures, uh, but it fails. Uh, the test set writes are actually not working. So you, you know, you probably want to write your own, uh, code, but it does give you some directions to try out, uh, reviewing PRs. It did really great, uh, particularly GPT-4 with web access. I was surprised by this that if you, if you pointed out to a particular PR, it will look at the code. It'll look for tests, it'll look for security issues. It'll give you useful feedback on like, does it match, does the code match? The description on the PR things like that. And that was super cool. Uh, there's one example I linked to in the blog of doing this and in the end, the, the PR review says just merge it, but we didn't because it doesn't have the context around the bigger issue it was trying to solve in superset. It's a little more, uh, wide in scope. So the, the approach the PR took was not the right one and it's being refactored. But still, as far as the code goes, it did a really good job of just kind of reviewing it. Uh, and then that's the web plugging browser working well, too. Yes. Yes. So that's that, that was pretty cool. Uh, then, yeah, making API calls, if you don't want to use supersets API layer in whatever language you're using for whatever the heck you're building, it doesn't okay job of scaffolding up an API call, but it doesn't know how to use the API. It can't read our documentation because it's rendered by the browser. It can't read the JSON file that's our swagger schema. It just doesn't know how to parse JSON at all. So it really doesn't know anything about our API and it says just insert a bunch of, you know, body here for your requests and that's not that helpful. I would rather just go to stack overflow for that. A similar thing with converting JavaScript to TypeScript. It doesn't have all the context it needs. It creates all sorts of unused types and undefined things and linting errors and, you know, you'd probably be better off just doing this yourself. And then creating tests for components, it did okay. It created some basic sanity checks like does this component explode? If I throw this prop at it, does it explode? But it doesn't really go deep on testing the functionality of any component. And it falters a bit on import paths and linting in TypeScript errors and things like that. And it can fix some of those TypeScript errors actually. But it doesn't know about supersets themes or these test crash and it creates assertions around non-existent DOM selectors that it just kind of dreamed up. And it doesn't always know what the component is really supposed to do like what its purpose is. It looks at the code a little bit in guesses. Yeah. Somebody's saying it did write the code for an API task. That's great. I would love to hear about that on the GitHub discussion. I'll paste a link to that again later. Then the auto generating docks strings. Max alluded to this one. And this works really well. It actually looks at the code and tells you, you know, what you put in, what you get out and gives a little bit of context. And that's pretty cool. So there's a PR open for this. I guess the downside is, again, somebody needs to fact check it. Fact check it. I don't know if it's dreaming this stuff up. I didn't look at it that closely. So it's kind of like you have to review all the code again. And the other question is, does this really, really help? Because if we did this for the whole code base, it's going to just inflate the code base quite a bit. And I worry about our signal to noise ratio in there. So jury's out. Look to hear some opinions. Well, anything can be generated on the fly. Should it live in a code repository, right? Since a little bit like, you know, if a tree falls in a forest. But, but, right, like, because then, because then it's just, you know, if your IDE could, you know, generate documentation on the fly or provide this context on the fly. Then let's just not put that in a code repo and inflate the code base and have something to maintain and manage, right? True, true. So yeah, I didn't try things like co-pilot and all of that too much. There's probably a lot more to talk about there. Then last, but not least, searching for relevant issues. This is, we have a lot of issues. And whether you're trying to open a new issue or considering opening one or you're writing a PR that may address some issues on the repo, I was hoping this would be a better search tool, particularly with the web browsing of GPT-4 turned on or Bards more, you know, up-to-date context. It didn't work at all. I got a couple of ancient links that were actually closed when I asked for open ones and got some very badly torn up documentation and it was just a fail. Then I'm not going to go into this stuff too much, but these are just things that I think have a bright future. If we got a little deeper and started playing around with APIs and so forth or prompt engineering and kind of tuning the model, things like that. So I hope that we can one day give a bit of sample data and say, I want a pie chart. Here's the form data schema for a pie chart so you can kind of pre-populate the controls for me and then get that back in a working state. I think that would be really cool. Organizing content using the front end and back end tagging systems that we now have in SuperSet as a feature flag, it would be really cool if it took your data sets, your charts, your dashboards and just kind of looked at them and said, this should have these tags so you could actually navigate your content more automatically. Content descriptions, similar sort of thing, but instead of tagging, actually kind of text-based descriptions of what's going on with a chart or a data set or a dashboard. And those could be used for all sorts of things like sending SMS messages, tweets, emails, Slack messages, things like that to tell you what's going on without having to have you interpret the visualizations. And then advanced types, this is another feature behind a feature flag where it kind of extends the semantic layer to say, this column is an IP address in one format and in another table you've got an IP address in a different format, but they're both IP addresses and it annotates them that way. I would imagine this would do a great job of looking at the schema of your database and kind of annotating those things for you so those relationships are created automatically. There's probably a lot more of these so I'd love to hear about all of that. The TLDR of all this stuff now that I've run through a bajillion use cases is I have trusted you. These things aren't good at everything. They suck at math, for example. And like I said, they tend to be compulsive liars. When I ask something they'll always give you an answer, they'll tell you what's correct and they'll even make up references. If you ask for references, it'll create web links that sound very realistic but never actually existed. And it'll send you on a fool's errand if you believe this stuff. You'll wind up trying to fix a PR or you'll wind up trying to patch some documentation and spend all this time fact checking when you could have just done it yourself and saved time. On the other hand, I've got the header wrong here but nonetheless, the technology is getting better and we're already seeing that with things like web access and a lot of the plugins. And many of these use cases do work. You saw some optimistic smiley faces on these and they can save you more time than you cost. You just have to be careful about the use cases you throw at it. And I think kind of start from a place of distrust. Just know how much work you're getting yourself into. And then getting into these API based integrations where you can control temperature and things like that and prompt engineering where you can make assertions around the prompts and the results you expect. I think that's got a bright future and kind of getting rid of some of the solutionation and dialing things in. So with that, I'll say a comment or two. One thing is interesting to see a take that is not like 100% bullish on AI. So it's refreshing to see something that's more critical of AI. I think it's interesting to look at in different areas with different fields, looking at experts and what they think of putting this thing to test. It asks to say a dentist, an expert dentist, like what their interaction around dentistry, working with AI has been like or a palantolog or a take different experts and see how it's doing things. One thing that's really clear to me is like you should use it as an assistant. You are the expert, you know SQL, you know your database schema, you know the code you're trying to write, you have the context of the project you're working on. Like you have a lot more context, hopefully you have more expertise. And it's just like a great assistant that works very quickly and well with you and you need to figure out what it's good at, what it's not good at and use it in that passion. So I think we're still in that phase where we are the professor and they are the assistant. Maybe that flips in a few weeks and a few months and a few years where it is the professor and your assistant. Then we shouldn't worry about other things at that point but use it in that way. That's probably one advice here. I think so. I hope so. It's still very early in this game and I'm still very bullish on all of this. But in the interest of time, I want to hand it over to Diego who's going to show us a little bit of magical integration work. Take it away. Hi, everyone. So I'm excited to share today how we're using AI in preset and specifically how we're using AI to generate SQL from from natural language. So you should be used to the SQL love interface here and probably you'll notice a new input here that we call AI assist. So the way it works is that you have to select the table or the tables that you think might be relevant for the query that you want to run. Now this is just to help the AI understanding the context and narrowing down the results. But we'll see how we can get to the point where the AI can just guess the tables that might be relevant to your query. So let's start with a very simple one that I have here. Retrieve all users who have CET listed as their time zone label and let's run this one. I hope it doesn't take too long. The AI needs to think sometimes. It's almost instant. Some other time you have to be a little bit patient here. Oh, it was really, really fast this time. Okay, that's cool. So we can see that the AI understood our intent and generated the query. So now there are a few interesting things here. The first thing is that we didn't actually mention the column name. So we just gave it a hint that we want the time zone label. So the AI currently guess that the column name is going to be a TZ label. And there is another interesting thing. So we asked for CET listed as their time zone. However, it's looking for central European time. And the way we are achieving this is by providing sample data to the AI and finding ways for prompt engineering to let the AI understand how to format the data in a way that it's actually going to work. So let's try this SQL and let's see if it actually runs. And it does. And we get the result set. Cool. Now let's try to be a little bit more heavy on the AI and ask for something a little bit more complex. So we're still trying to get all the users we've seen to listed as their time zone. But we also want to get the channels that they have created. Now, the thing that I want you to know this year is that we aren't actually preselecting any other table than the users one. And though we're asking for probably another table here, let's see what the AI thinks about this. It's thinking. So it's the one thing to you know, the calls to open or open AI or the different AI as API is not as often not sub second for like the test we've done depending on the amount of context and depending on how much text you're asking it to generate. And I take like one seven seconds per call and depending on the model you use, I used GVD5 turbo like 35 turbo. It's probably a lot faster than you know, your GBT floor. So that's an area to where we're going to see a lot of better answers faster to and get more of that Google like millisecond type of expectation that Google search has been like you know, notoriously milliseconds for a while. I think we're going to see that too on a lot of these, yeah, I serve LLM services. Yeah, absolutely. All right, so you got the query generated and it looks just fine. As we can see there are a few things to notice here. So first of all, it was able to currently guess that we need the channel stable here. I mean, this wasn't really hard because I mentioned the channel stable. However, we also preselected the table here for convenience. So that's just automatically happening for you. And another thing that's interesting here is that we didn't specify which column from the channel's table is actually connecting the users as creators of those channels. So and here it's actually understanding our intent and inferring the column name from the table and using it in the query, which looks absolutely fine. Now I want you to notice one more thing. We have this little icon here, this light bulb. And what the I's doing here is giving us suggestions and ways to improve our input, our intent so that it's clear and better results can be returned. So here it's saying that it's unclear whether we want archived channels or not and we can improve our input to provide more information about that. So it knows the context around your database schema and tries to give you hints and suggestions of how to improve your input if you want to. Now let's try one more. So we are slowly adding more complexity. And here we are asking for the full names of all users and we are still asking for the Channels as well. And one of the things too, if you think about the design of this feature, it's very step by step in some ways where we show, so there's your prompt. Then we show you the SQL, we show you the tables that you've, that have been used for context and we let you run the selection and what was generated. And I think that ties to Evans, kind of mistrust or not blind, something short of like blind trust because you could just ask a question tell, tell me how my sales are doing and the world right now, my company is doing and runs a bunch of SQL and tells you that your company is doing great or the answer is 6.5%. And you probably would interest that. So I think in terms of UX, it does make sense for us to show you the SQL, show you the table, let you run it, look at the results set and put that in context. As our confidence grows better, but AI, we might be able to get to UI, UX's that are much more deterministic and just like ask a plan question and we'll show you a chart. But I think now, this is about the right level of trust we have for the UI and the right level of like trace leaving the right trace behind for people to validate. Yeah. So I want to stress a little bit more the fact of how good is it in varying the right names, the right columns from the table schema just by understanding the semantic similarity. So we are asking for the full names, we need to specify which column is actually hosting those names and the AI is looking at the sample that I were giving and understands that the actual column name that is hosting the full names is the real name and it's using that column for that. So there are also a few more toys that we are implementing here which are more on the interaction side. We want this process to be an iterative process for you. So as you can see, as soon as you run the natural language query, you are getting a new SQL appended to the existing ones. So in the SQL editor, you see all the progress of your query. So if you want to compare, you can do that and you can easily also compare the data that you're getting. And let's say that you want to overwrite the query running immediately, you can also do that. Probably this will not return and result because there is no users that have this specific feature, but I can show you that the AI will think about the query, think about intent, generate the SQL and then the SQL app will immediately highlight the query and run it for you on the fly. And that's it. So that's all for me. I hope you find interesting and magical as much as we do. And if you have any questions, feel free to ask. Thank you so much. Yeah, I want to talk a little bit about what's going on behind the scene too because there's been a lot of questions on that and then welcoming people to the GitHub conversation too, where I have a little bit of a segment to that I'll present some use cases around, you know, more like a your data explorer, your data analyst, you're trying to build a dashboard. So I have some examples that I'll get into in just a second. But in terms of what's happening behind the scene, what's we're interacting with LM? I know there's a lot of questions on like which LM to use. I think we haven't really settled. Right now we're testing at scale. Like what is the best model we can use between open AI, Bard, should it be GP3535 turbo or four? So that I think is like subject to change. I don't know for this particular demo what was this the version we're using, but we know this stuff is changing so fast that we don't want to commit too deeply to a particular model. Some of what's interesting of what's happening behind the scene is what we call what I call prompt what I guess the industry is getting to call from engineering, right? So that means we pass what's happening in prompt engineering is we craft a message, right? So the message, one thing that's really important about the message is what wasn't that top bar that Diego was showing now there's much more that goes into it. You know, we're like we say things like a you're an AI that's an expert at SQL. That is looking to answer the following question. Now here's the current context of the database you're on to first say it's a BigQuery database. We're going to have to speak in this specific dialect. Now here's some tables that the users thought having context right now. And here's some more tables from the schema that's currently selected. And then if there's more room in the prompt window, here's even more tables that we think might be relevant, right? So I think they're the challenge in prompt engineering is working with a limited window. Depending on the model you use it's like 2000, 4000, you know, 8000 maybe 32,000 token at open AI. Now there's new models with larger token windows that did so we can pass can pass it more context. While also looking at can we train the model with your data or with with the schema information that we know about is that a good option right to fine tune a model and teach it more about your specific database schemas and all the metadata you might have. There's a vector databases we can use to retrieve the information so we're testing all these things to provide the best kind of magical experience there. With that and then if you have more questions on this too, I would say like that's the work we're doing behind the scene that is rapidly evolving to. And I think for now to be interesting to I think we're looking to keep that feature as a preset kind of sprinkle differentiator to so you know as a commercial open source company we need to build some differentiators and we think AI is a good use case for for that. So you can try the you'll be able to try these features on on preset very soon. And with that I'm going to do a little bit more we have about what 13 minutes left so we can leave a little bit more time to explore the Q and A questions. Quite a bit there. Yeah. And maybe I'll try to go quickly. The stuff I prefer I prepared this morning fairly quickly too. So there's not that much to show but what I want to show is more like a your data practitioner you're not necessarily you don't necessarily define yourself as a superset user right. So you're probably a member of a data team somewhere data engineer your data analysts you're a data PM maybe are you someone who plays with data some of the time. So I wanted to show us just some prompts and some ideas as to how you can use AI or chat GPT in this case is an assistant and some of the jobs you're doing every day. So the first thing I wanted to highlight is chat GPT or GPT models like really know about the different types of visualization and when to use them. I get knows about data is theory and what different types of charts are good at conveying right. So you're crafting the visualization you're trying to tell a story or crafting a dashboard. It can help you with that. This is from a session earlier but here is a different type of prompt. And you can imagine to that for any of these prompts based on the answer you can go deeper into different components of the answer. But here is like a I've been tested design a dashboard for the CEO of my startup. I'm wondering where to start. I know about a few key metrics and dimensions but I don't know what the CEO cares most about. Can you help me prepare a series of questions and to ask and maybe roles around the CEO to target with those questions right. So here it's like, well, here's something people you might want to talk to. Here are some questions or areas you might want to explore with these people. And the point here is if you wanted to play data PM and dig a little deeper like, yeah, I want to prepare a session, a 10 minute session with the CFO to ask about some the key indicators or I want to prepare an interview for the CEO about the dashboard at once. Can you help me structure and prepare some questions right. So just saying it can help you think through some of these things and help you plan as a dashboard designer. So that was an example of a follow up question that's scheduled time with the CFO can you list out a few questions like a disaster in the session asking about key metrics, APIs, you know, performance goals like what is important to you as a CFO this time granularity, you know, forecast and all sorts of information. Now I had to follow up in this idea around this area. Okay, I've identified a lot of key metrics dimensions and the kind of exploration I want to offer in my dashboard, how should I go about, you know, layout and structuring my dashboard. So it talks about prioritizing metrics, dividing the dashboard into sections, think about low. So it's, you know, here, here it's really helping you structure how you should think about approaching a complex task and maybe breaking it down into smaller things. And then for any of these things, it might be like, I might be able to go and roll with that or I don't think this is important. I think this is important. Or let's break this subtask further. So just thinking like, you know, it can be a really good. I was going to say assistant, but it's going to advisor around some of these things. Here I had more of the different use case that's around, okay, I'm a data analyst or a data analyst, I'm wearing that hat right now. And then I'm asked with telling the story of how our and our are and I probably would not have had to specify here, but met revenue retention. I just carry record over the last month. How do I approach breaking this down and figuring out where this trend is coming from? Right. So pretty abstract task. Enclared your unclear way to start. So here, you know, which I GPT providing a lot of like decomposing this metric of NRR and sub components, expansion, pricing changes, degrees and chairs, suggestions, some areas to look deeper into. And here I'm getting more into, okay, the data said I'm working with as a certain, you know, shape and form. Where should I start? What is some sequel I could run? I think here I went back where I was like, oh, actually, I don't have this per-taple. If I did have that table and I had just like an ARR table, which is revenue of customers over time, how would I get to that table that I said I had earlier? So it's helping with all of that writing some complex sequel, which I reviewed and seem very sane. You know, just more on that session. Like how, here I get a little bit deeper into, okay, now I'm figuring stuff out. I found this and now how do I go and tell that story and produce, you know, a document or a story of a certain kind. So here, just saying like as a data practitioner, you might be left alone and like not knowing where to start. Like having an advisor and assistant or someone to go back and forth what ideas. And then, you know, seeing like a really good thing. And on the meta side, maybe as a last thought there is like really often people will come up and ask a very simple prompt of like, hey, can you help me figure out why, you know, this metric is going up and then it says something that I'm interested in. You might just give up and be like, okay, I'm on my own. This was not that helpful. But I encourage people to go and just ask deeper. It's like, hey, you said this. This is right. This was useful. Another stuff you said was not useful at all. Let me provide a little bit more context. Can you help me now? And then generally, like take it from there and I keep the conversation going. And you're very likely to have, you know, some help, some ideas, unblock yourself and really good ways. All right. Thank you. Appreciate the perspective. So we've got a lot of questions. Too many questions for the seven minutes we have left. So I just want to reiterate that on the Super Set community Slack, there is a channel called Meetup-GBT. You can feel free to start some threads there. If you have some questions you want to get to that don't get addressed here, there's also even on that channel, there's a link to the GitHub discussion where if you have more ideas of things to try experiments, you'd like to see run, put them there and we're happy to keep diving into this. We're as excited as anyone. So some quick questions. One was what version of GPT did I use in the examples and the answer was basically 3.5 and 4. Just going back and forth to kind of see how they differ. I was trying to use them and barred in most cases. And on a related note, somebody asks if we're, if we've tried perplexity, I have not. I don't know if either of you guys have, but that's the perfect thing to drop in the GitHub discussion and we absolutely will. And then yeah, I don't know if maybe there's a future in which you create a prompt and you send it to all of these different tools and just shotgun the ecosystem and see what you get back. That'd be an interesting question. Yes, then what do you think about each other? It's like, okay, can you tell me off the answers that are not yours which one preferred sensor like this? Yeah. Then one person said that they actually got the API prompt to behave and generate some actual request headers and so forth. That's fantastic. I was not able to. Maybe it depends on the complexity of the endpoint. Maybe it depends on how completionist the documentation out there in the world as of a certain timeframe was. I don't know, but I'll kick the tires some more on that. Again, these hope ratings are my subjective opinion and I would love to get more hot tips on how to dial this stuff in because that'll help me out a lot. I assure you. Here's a thought too. It's like we've been writing documentations for users forever, right? But I think like writing documentations for AI is really great now so that you know, maybe the driver, the real reason why we write documentation now so that I can pick it up. So I think the conversation and I'd go back and forth and much more directed. Blow to answer more direct questions from the documentation. Yeah, how would we write differently if we were writing for these to parse? What is the machine language for? I think it's the same, which is the beauty, right? So it's just like now the incentive to write documentation is to teach the user but also the AI in the same way and then the user can go to the documentation but it's more likely to go to a chatbot that can make sense of things and point the right place. Yeah, we've got a question around when the first, you know, AI assist version might hit the streets. I don't think we're ready to shoot ourselves in the foot with the date but we're working on it. This is a sneak preview. This is the first time the world's seen it. Yeah, if you want to be an early adopter, I think we'll probably have like real beta soon. So if your preset user just send your email address and you can add you to the list, we should have a wait list like page where people can just say like, oh, can I be an early adopter? The same way that all the AI tools do now where like, oh, can I be on the wait list for plugins and for the different AI features? Right. We've got a lot of questions about is this going to be available in open source and the answer is basically no. This is going to be a preset feature. But open source open source, right? So I want to welcome to go and contribute it. The part of reason why, you know, we're not contributing back to open source. There's a lot of stuff is very unsettled and it's going to change very quickly, right? So if you make a choice to, I don't know, tightly couple use, I don't know, chain and couple with a certain, with open AI, it could be that, you know, a month from now, barred is a much better choice. Or there's a, you know, some open source model with completely different approach. So if, if felt to us that we're working on unsettled grounds and, you know, to contribute to open source very likely that we'll start using stuff a certain way and it'd be difficult to upgrade or change things because it might be better or worse in intricate ways too. So to unsettle to contribute in a permanent kind of way to superset, but we certainly would not, you know, prevent people from contributing that direction if they have a plan and there's consensus in the community too. I would also wager that anyone that contributes to be through like this to the open source core is going to find a lot of things are controversial. Now we implement it. What we do is it is going to have different implications for people's businesses and all sorts of things. So this gives us an opportunity to move quickly in the way that we're opinionated about where other people may have different opinions. And when you do that in open source, it becomes a very large. I think we haven't talked about too. So this is like just one take on bringing AI to a data tool, right? And in this case, like SQL generation. Like I think there's a really interesting maybe that's for our follow up podcast of like how are other ways that we could bring AI and to and to superset our data workflows in general. And I think like one that we've talked about at preset and in the community to is like a chart suggestion engine or a dashboard, you know, auto magic dashboard generation type tool. Okay. I have a data set, I have a result set of some some kind. I'd like to get some charts suggested suggested to. There's also like the use case of I'd like to have a conversation with a chat, but about my data, right? And then for the chat, but to be able to generate visualization to the right, say, well, run it, present you with things and to have this this more direct conversation. Those are like probably in many ways like more more useful to than a SQL generation, you know, tool or a bot and things that, you know, would be really interesting to contribute to superset or to add into preset. We're essentially out of time. Do you guys see anything in the Q and A tab that you want to address before we hang up? Somebody's asking if this is going to be available later and it will be on YouTube in the day or two. So check the preset YouTube channel. Yeah, the blog post the blog post is up. The that will be this will be on YouTube. If you are interested to and I'm not sure what's the best way for people to signal probably on a superset's like of like, hey, we'd like to go deeper into this content. Other ideas for the podcast to our webinars, always welcome. So if you're like, oh, I showed up because I wanted to hear about acts, you know, do a lot more detail. Tell us like we love to have these sessions. Yes, this is the first time we've tried this experiment of setting up a Slack channel specifically for a meetup. So that meetup dash GPGPT channel is specifically to keep this conversation going. So if you had a question that wasn't addressed today, please drop it there and we'll do our best. All right. Well, thank you all for joining us today. Thanks for the spirited conversation and we look forward to seeing where this all goes in the future. Until next time. At the AI everybody. Thanks everyone. Thanks for hosting.